# C4 ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå›³ - Pythonç‰ˆ

**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0.0  
**ä½œæˆæ—¥**: 2025-10-27  
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: Draft  
**å®Ÿè£…è¨€èª**: Python 3.11

---

## ğŸ“‹ æ¦‚è¦

C4ãƒ¢ãƒ‡ãƒ«ã®ãƒ¬ãƒ™ãƒ«3ã¨ã—ã¦ã€Pythonç‰ˆAzure Functionsã®å†…éƒ¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ§‹é€ ã‚’è©³ç´°ã«ç¤ºã—ã¾ã™ã€‚

**å¯¾è±¡ç¯„å›²**:
- FileDetection Functionï¼ˆPythonå®Ÿè£…ï¼‰
- DataTransform Functionï¼ˆPythonå®Ÿè£…ï¼‰
- FileUploader Functionï¼ˆPythonå®Ÿè£…ï¼‰
- JobMonitor Functionï¼ˆPythonå®Ÿè£…ï¼‰
- å…±æœ‰ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆShared Modulesï¼‰

---

## ğŸ—ï¸ Pythonç‰ˆå…¨ä½“æ§‹æˆ

### ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```
src/python/
â”œâ”€â”€ functions/
â”‚   â”œâ”€â”€ file_detection/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ function_app.py          # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
â”‚   â”‚   â”œâ”€â”€ handler.py                # ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â”‚
â”‚   â”œâ”€â”€ data_transform/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ function_app.py
â”‚   â”‚   â”œâ”€â”€ handler.py
â”‚   â”‚   â”œâ”€â”€ transformer.py            # å¤‰æ›ã‚¨ãƒ³ã‚¸ãƒ³
â”‚   â”‚   â”œâ”€â”€ mapper.py                 # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ”ãƒ³ã‚°
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â”‚
â”‚   â”œâ”€â”€ file_uploader/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ function_app.py
â”‚   â”‚   â”œâ”€â”€ handler.py
â”‚   â”‚   â”œâ”€â”€ client.py                 # CSV Upload APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
â”‚   â”‚   â”œâ”€â”€ metadata_builder.py       # metadata.jsonç”Ÿæˆ
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â”‚
â”‚   â””â”€â”€ job_monitor/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ function_app.py
â”‚       â”œâ”€â”€ handler.py
â”‚       â”œâ”€â”€ reporter.py               # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
â”‚       â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ shared/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ sds_models.py             # SDSãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«
â”‚   â”‚   â”œâ”€â”€ oneroster_models.py       # OneRosterãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«
â”‚   â”‚   â””â”€â”€ job_models.py             # ã‚¸ãƒ§ãƒ–ç®¡ç†ãƒ¢ãƒ‡ãƒ«
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ logger.py                 # æ§‹é€ åŒ–ãƒ­ã‚°
â”‚   â”‚   â”œâ”€â”€ azure_client.py           # Azure SDKçµ±åˆ
â”‚   â”‚   â”œâ”€â”€ csv_parser.py             # CSVæ“ä½œ
â”‚   â”‚   â””â”€â”€ validators.py             # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
â”‚   â”‚
â”‚   â”œâ”€â”€ constants.py                  # å®šæ•°å®šç¾©
â”‚   â””â”€â”€ config.py                     # è¨­å®šç®¡ç†
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_file_detection.py
â”‚   â”‚   â”œâ”€â”€ test_data_transform.py
â”‚   â”‚   â”œâ”€â”€ test_file_uploader.py
â”‚   â”‚   â””â”€â”€ test_shared_utils.py
â”‚   â”‚
â”‚   â””â”€â”€ integration/
â”‚       â”œâ”€â”€ test_e2e_flow.py
â”‚       â””â”€â”€ test_api_integration.py
â”‚
â”œâ”€â”€ requirements.txt                  # å…±é€šä¾å­˜é–¢ä¿‚
â”œâ”€â”€ host.json                         # Function Appè¨­å®š
â”œâ”€â”€ local.settings.json              # ãƒ­ãƒ¼ã‚«ãƒ«è¨­å®šï¼ˆGité™¤å¤–ï¼‰
â””â”€â”€ README.md
```

### ä¸»è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

```python
# requirements.txt
azure-functions==1.18.0
azure-storage-blob==12.19.0
azure-data-tables==12.4.0
azure-identity==1.15.0
azure-keyvault-secrets==4.7.0

pandas==2.1.4
numpy==1.26.2

requests==2.31.0
pydantic==2.5.3
python-dotenv==1.0.0

pytest==7.4.3
pytest-cov==4.1.0
pytest-asyncio==0.23.2
```

---

## ğŸ”§ FileDetection Functionï¼ˆPythonç‰ˆï¼‰

### ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ§‹é€ 

```mermaid
graph TD
    A[Event Grid Trigger] --> B[function_app.py]
    B --> C[handler.py]
    
    C --> D[BlobValidator]
    C --> E[FileCompletenessChecker]
    C --> F[JobInitializer]
    
    D --> G[shared.validators]
    E --> H[shared.azure_client]
    F --> I[shared.models.job_models]
    
    H --> J[Blob Storage Client]
    H --> K[Table Storage Client]
    
    C --> L[shared.logger]
    L --> M[Application Insights]
    
    style B fill:#90EE90
    style C fill:#87CEEB
    style D fill:#FFB6C1
    style E fill:#FFB6C1
    style F fill:#FFB6C1
```

### ã‚¯ãƒ©ã‚¹è¨­è¨ˆ

#### 1. function_app.py
```python
import azure.functions as func
import logging
from .handler import FileDetectionHandler

app = func.FunctionApp()

@app.event_grid_trigger(arg_name="event")
def file_detection(event: func.EventGridEvent):
    """
    Blob Storage Event Gridãƒˆãƒªã‚¬ãƒ¼
    ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¤œçŸ¥
    """
    logging.info(f"Event Grid trigger: {event.id}")
    
    handler = FileDetectionHandler()
    result = handler.handle(event)
    
    return result
```

#### 2. handler.py
```python
from typing import Dict, List
from azure.functions import EventGridEvent
from shared.utils.azure_client import AzureStorageClient, AzureTableClient
from shared.utils.logger import StructuredLogger
from shared.utils.validators import FileValidator
from shared.models.job_models import Job, JobStatus
from shared.constants import REQUIRED_FILES

class FileDetectionHandler:
    """ãƒ•ã‚¡ã‚¤ãƒ«æ¤œçŸ¥ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    
    def __init__(self):
        self.blob_client = AzureStorageClient()
        self.table_client = AzureTableClient()
        self.logger = StructuredLogger("FileDetection")
        self.validator = FileValidator()
    
    def handle(self, event: EventGridEvent) -> Dict:
        """
        ãƒ¡ã‚¤ãƒ³å‡¦ç†
        
        Args:
            event: Event Gridã‚¤ãƒ™ãƒ³ãƒˆ
            
        Returns:
            å‡¦ç†çµæœ
        """
        try:
            # ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿è§£æ
            blob_url = event.data['url']
            directory = self._extract_directory(blob_url)
            
            self.logger.info("File detection started", {
                "directory": directory,
                "blob_url": blob_url
            })
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
            files = self._check_file_completeness(directory)
            
            if not files:
                self.logger.warning("Files not complete yet", {
                    "directory": directory
                })
                return {"status": "waiting"}
            
            # ã‚¸ãƒ§ãƒ–åˆæœŸåŒ–
            job = self._initialize_job(directory, files)
            
            # å¤‰æ›Functionãƒˆãƒªã‚¬ãƒ¼
            self._trigger_transform(job)
            
            self.logger.info("File detection completed", {
                "job_id": job.job_id,
                "file_count": len(files)
            })
            
            return {"status": "success", "job_id": job.job_id}
            
        except Exception as e:
            self.logger.error("File detection failed", {
                "error": str(e)
            }, exc_info=True)
            raise
    
    def _extract_directory(self, blob_url: str) -> str:
        """
        Blob URLã‹ã‚‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹æŠ½å‡º
        ä¾‹: .../sds-csv-input/20251027/school.csv â†’ 20251027
        """
        parts = blob_url.split('/')
        container_idx = parts.index('sds-csv-input')
        return parts[container_idx + 1]
    
    def _check_file_completeness(self, directory: str) -> List[str]:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
        
        å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ç®¡ç†ã®ã¿ï¼‰:
        - school.csv
        - student.csv
        - teacher.csv
        
        ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚¯ãƒ©ã‚¹ç®¡ç†ï¼‰:
        - section.csv
        - studentenrollment.csv
        - teacherroster.csv
        """
        prefix = f"sds-csv-input/{directory}/"
        blobs = self.blob_client.list_blobs(
            container='sds-csv-input',
            prefix=prefix
        )
        
        file_names = [blob.name.split('/')[-1] for blob in blobs]
        
        # å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯
        required = {'school.csv', 'student.csv', 'teacher.csv'}
        if not required.issubset(set(file_names)):
            return []  # ã¾ã æƒã£ã¦ã„ãªã„
        
        return file_names
    
    def _initialize_job(self, directory: str, files: List[str]) -> Job:
        """ã‚¸ãƒ§ãƒ–åˆæœŸåŒ–"""
        from datetime import datetime
        import uuid
        
        timestamp = datetime.utcnow()
        job_id = f"job-{timestamp.strftime('%Y%m%d-%H%M%S')}-{uuid.uuid4().hex[:8]}"
        
        job = Job(
            partition_key=timestamp.strftime('%Y-%m'),
            row_key=job_id,
            job_id=job_id,
            status=JobStatus.PROCESSING,
            start_time=timestamp,
            input_directory=f"sds-csv-input/{directory}/",
            input_files=files,
            user_id="system",  # å®Ÿéš›ã¯Azure ADã‹ã‚‰å–å¾—
            version="python"
        )
        
        # Table Storageã«ä¿å­˜
        self.table_client.insert_entity('JobHistory', job.to_dict())
        
        return job
    
    def _trigger_transform(self, job: Job):
        """ãƒ‡ãƒ¼ã‚¿å¤‰æ›Functionã‚’ãƒˆãƒªã‚¬ãƒ¼"""
        import requests
        import os
        
        transform_url = os.getenv('TRANSFORM_FUNCTION_URL')
        
        payload = {
            "job_id": job.job_id,
            "input_directory": job.input_directory,
            "files": job.input_files
        }
        
        response = requests.post(
            transform_url,
            json=payload,
            headers={'Content-Type': 'application/json'}
        )
        
        response.raise_for_status()
```

---

## ğŸ”„ DataTransform Functionï¼ˆPythonç‰ˆï¼‰

### ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ§‹é€ 

```mermaid
graph TD
    A[HTTP Trigger] --> B[function_app.py]
    B --> C[handler.py]
    
    C --> D[CSVReader]
    C --> E[DataValidator]
    C --> F[DataTransformer]
    C --> G[CSVWriter]
    
    D --> H[pandas]
    E --> I[shared.validators]
    F --> J[mapper.py]
    
    J --> K[SDSToOneRosterMapper]
    K --> L[GUIDGenerator]
    K --> M[FieldMapper]
    
    G --> N[shared.azure_client]
    N --> O[Blob Storage]
    
    C --> P[shared.logger]
    
    style B fill:#90EE90
    style C fill:#87CEEB
    style F fill:#FFD700
```

### ã‚¯ãƒ©ã‚¹è¨­è¨ˆ

#### transformer.py
```python
import pandas as pd
from typing import Dict, List
from shared.models.sds_models import SDSSchool, SDSStudent, SDSTeacher
from shared.models.oneroster_models import OneRosterOrg, OneRosterUser
from shared.utils.logger import StructuredLogger

class DataTransformer:
    """SDS â†’ OneRoster ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.logger = StructuredLogger("DataTransformer")
        self.mapper = SDSToOneRosterMapper()
    
    def transform_schools(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        school.csv â†’ orgs.csv å¤‰æ›
        
        SDSãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰:
        - School SIS ID
        - Name
        - School Number
        - School NCES_ID (optional)
        - State ID (optional)
        
        OneRosterãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰:
        - sourcedId (GUID)
        - status (active)
        - dateLastModified
        - name
        - type (school)
        - identifier (School SIS ID)
        """
        self.logger.info(f"Transforming schools: {len(df)} records")
        
        transformed = []
        for _, row in df.iterrows():
            org = self.mapper.map_school_to_org(row)
            transformed.append(org.to_dict())
        
        result_df = pd.DataFrame(transformed)
        return result_df
    
    def transform_users(
        self,
        students_df: pd.DataFrame,
        teachers_df: pd.DataFrame
    ) -> pd.DataFrame:
        """
        student.csv + teacher.csv â†’ users.csv å¤‰æ›
        """
        self.logger.info(
            f"Transforming users: {len(students_df)} students, "
            f"{len(teachers_df)} teachers"
        )
        
        users = []
        
        # å­¦ç”Ÿå¤‰æ›
        for _, row in students_df.iterrows():
            user = self.mapper.map_student_to_user(row)
            users.append(user.to_dict())
        
        # æ•™å“¡å¤‰æ›
        for _, row in teachers_df.iterrows():
            user = self.mapper.map_teacher_to_user(row)
            users.append(user.to_dict())
        
        result_df = pd.DataFrame(users)
        return result_df
```

#### mapper.py
```python
import uuid
from datetime import datetime
from shared.models.sds_models import SDSSchool
from shared.models.oneroster_models import OneRosterOrg

class SDSToOneRosterMapper:
    """SDSã¨OneRosterã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°"""
    
    def __init__(self):
        self._guid_cache = {}  # GUIDå†åˆ©ç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    
    def map_school_to_org(self, sds_row: dict) -> OneRosterOrg:
        """
        school.csv â†’ orgs.csv ãƒãƒƒãƒ”ãƒ³ã‚°
        
        å¤‰æ›ãƒ«ãƒ¼ãƒ«:
        1. sourcedId: SDS ID ã‹ã‚‰GUIDç”Ÿæˆï¼ˆæ±ºå®šçš„ï¼‰
        2. name: Name ã‚’ãã®ã¾ã¾ä½¿ç”¨
        3. type: å›ºå®šå€¤ "school"
        4. identifier: School SIS ID
        """
        sds_id = str(sds_row['School SIS ID'])
        
        # æ±ºå®šçš„GUIDç”Ÿæˆï¼ˆåŒã˜IDãªã‚‰åŒã˜GUIDï¼‰
        sourced_id = self._generate_deterministic_guid(
            'school',
            sds_id
        )
        
        org = OneRosterOrg(
            sourcedId=sourced_id,
            status='active',
            dateLastModified=datetime.utcnow().isoformat() + 'Z',
            name=sds_row['Name'],
            type='school',
            identifier=sds_id,
            # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
            metadata={
                'schoolNumber': sds_row.get('School Number'),
                'ncesId': sds_row.get('School NCES_ID'),
                'stateId': sds_row.get('State ID')
            }
        )
        
        return org
    
    def map_student_to_user(self, sds_row: dict) -> 'OneRosterUser':
        """student.csv â†’ users.csv ãƒãƒƒãƒ”ãƒ³ã‚°"""
        sds_id = str(sds_row['SIS ID'])
        
        sourced_id = self._generate_deterministic_guid(
            'student',
            sds_id
        )
        
        # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ç”Ÿæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
        email = sds_row.get('Username') or f"{sds_id}@example.edu"
        
        user = OneRosterUser(
            sourcedId=sourced_id,
            status='active',
            dateLastModified=datetime.utcnow().isoformat() + 'Z',
            enabledUser=True,
            username=sds_row.get('Username', sds_id),
            userIds=[{
                'type': 'SIS',
                'identifier': sds_id
            }],
            givenName=sds_row.get('First Name', ''),
            familyName=sds_row.get('Last Name', ''),
            middleName=sds_row.get('Middle Name'),
            role='student',
            identifier=sds_id,
            email=email,
            sms=sds_row.get('Phone'),
            phone=sds_row.get('Phone'),
            # ã‚ªãƒ—ã‚·ãƒ§ãƒ³: çµ„ç¹”æ‰€å±
            orgs=[self._get_org_guid(sds_row.get('School SIS ID'))],
            grades=[sds_row.get('Grade')]
        )
        
        return user
    
    def _generate_deterministic_guid(
        self,
        entity_type: str,
        entity_id: str
    ) -> str:
        """
        æ±ºå®šçš„GUIDç”Ÿæˆ
        åŒã˜å…¥åŠ›ãªã‚‰å¸¸ã«åŒã˜GUIDã‚’è¿”ã™ï¼ˆUUID v5ä½¿ç”¨ï¼‰
        
        Args:
            entity_type: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚¿ã‚¤ãƒ—ï¼ˆschool, student, teacherç­‰ï¼‰
            entity_id: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£IDï¼ˆSDS IDï¼‰
            
        Returns:
            UUIDå½¢å¼ã®æ–‡å­—åˆ—
        """
        cache_key = f"{entity_type}:{entity_id}"
        
        if cache_key in self._guid_cache:
            return self._guid_cache[cache_key]
        
        # åå‰ç©ºé–“UUIDã‚’ä½¿ç”¨ï¼ˆOneRosterå°‚ç”¨ï¼‰
        namespace = uuid.UUID('6ba7b810-9dad-11d1-80b4-00c04fd430c8')
        
        # entity_type + entity_id ã‹ã‚‰æ±ºå®šçš„ã«GUIDç”Ÿæˆ
        guid = str(uuid.uuid5(namespace, cache_key))
        
        self._guid_cache[cache_key] = guid
        return guid
    
    def _get_org_guid(self, school_sis_id: str) -> str:
        """çµ„ç¹”GUIDã‚’å–å¾—ï¼ˆå­¦æ ¡IDã‹ã‚‰ï¼‰"""
        if not school_sis_id:
            return None
        return self._generate_deterministic_guid('school', str(school_sis_id))
```

---

## ğŸ“¤ FileUploader Functionï¼ˆPythonç‰ˆï¼‰

### ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ§‹é€ 

```mermaid
graph TD
    A[HTTP Trigger] --> B[function_app.py]
    B --> C[handler.py]
    
    C --> D[MetadataBuilder]
    C --> E[CSVUploadAPIClient]
    C --> F[RetryHandler]
    
    D --> G[metadata_builder.py]
    G --> H[hashlib SHA-256]
    
    E --> I[client.py]
    I --> J[requests]
    I --> K[Azure Identity]
    
    F --> L[exponential_backoff]
    
    C --> M[shared.azure_client]
    M --> N[Blob Storage]
    M --> O[Table Storage]
    M --> P[Key Vault Client]
    
    style B fill:#90EE90
    style C fill:#87CEEB
    style E fill:#FF6347
```

### ã‚¯ãƒ©ã‚¹è¨­è¨ˆ

#### metadata_builder.py
```python
import hashlib
import json
from typing import Dict, List
from datetime import datetime
from shared.utils.azure_client import AzureBlobClient
from shared.utils.logger import StructuredLogger

class MetadataBuilder:
    """metadata.jsonç”Ÿæˆ"""
    
    def __init__(self):
        self.blob_client = AzureBlobClient()
        self.logger = StructuredLogger("MetadataBuilder")
    
    def build_metadata(
        self,
        output_directory: str,
        file_list: List[str]
    ) -> Dict:
        """
        ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSONç”Ÿæˆ
        
        Args:
            output_directory: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
            file_list: CSVãƒ•ã‚¡ã‚¤ãƒ«åãƒªã‚¹ãƒˆ
            
        Returns:
            ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒª
        """
        self.logger.info("Building metadata", {
            "directory": output_directory,
            "file_count": len(file_list)
        })
        
        # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚§ãƒƒã‚¯ã‚µãƒ ã¨ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã‚’è¨ˆç®—
        checksums = {}
        record_counts = {}
        
        for filename in file_list:
            blob_path = f"{output_directory}/{filename}"
            
            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            content = self.blob_client.download_blob(blob_path)
            
            # SHA-256ãƒã‚§ãƒƒã‚¯ã‚µãƒ è¨ˆç®—
            checksum = hashlib.sha256(content.encode('utf-8')).hexdigest()
            checksums[filename] = checksum
            
            # ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã‚«ã‚¦ãƒ³ãƒˆï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼é™¤ãï¼‰
            lines = content.strip().split('\n')
            record_count = max(0, len(lines) - 1)  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’é™¤ã
            record_counts[filename] = record_count
            
            self.logger.debug(f"File processed: {filename}", {
                "checksum": checksum[:16] + "...",
                "records": record_count
            })
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
        metadata = {
            "source": "SDS2Roster",
            "version": "1.0.0",
            "uploadedAt": datetime.utcnow().isoformat() + "Z",
            "recordCounts": record_counts,
            "checksums": checksums
        }
        
        self.logger.info("Metadata built successfully", {
            "total_records": sum(record_counts.values()),
            "total_files": len(file_list)
        })
        
        return metadata

#### client.py
```python
import requests
from typing import Dict, List, Optional
from azure.identity import ManagedIdentityCredential
from shared.utils.azure_client import AzureKeyVaultClient
from shared.utils.logger import StructuredLogger
import time

class CSVUploadAPIClient:
    """CSV Upload API v1ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self):
        self.kv_client = AzureKeyVaultClient()
        self.credential = ManagedIdentityCredential()
        self.logger = StructuredLogger("CSVUploadAPIClient")
        self.session = requests.Session()
        self._token_cache: Optional[Dict] = None
    
    def upload_csv_files(
        self,
        files: Dict[str, bytes],
        metadata: Dict
    ) -> Dict:
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ä¸€æ‹¬ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        
        Args:
            files: ãƒ•ã‚¡ã‚¤ãƒ«å -> ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒª
            metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒª
            
        Returns:
            APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆuploadIdå«ã‚€ï¼‰
        """
        # Key Vaultã‹ã‚‰APIè¨­å®šå–å¾—
        api_endpoint = self.kv_client.get_secret('upload-api-endpoint')
        api_key = self.kv_client.get_secret('upload-api-key')
        
        # Azure AD Bearer Tokenå–å¾—
        bearer_token = self._get_bearer_token()
        
        url = f"{api_endpoint}/upload"
        
        headers = {
            'Authorization': f'Bearer {bearer_token}',
            'X-API-Key': api_key
        }
        
        # multipart/form-dataã®æ§‹ç¯‰
        files_data = []
        
        # metadata.jsonã‚’è¿½åŠ 
        import json
        files_data.append((
            'files',
            ('metadata.json', json.dumps(metadata), 'application/json')
        ))
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ 
        for filename, content in files.items():
            files_data.append((
                'files',
                (filename, content, 'text/csv')
            ))
        
        self.logger.info("Uploading CSV files", {
            "file_count": len(files),
            "endpoint": url
        })
        
        # ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ä»˜ãé€ä¿¡
        response = self._post_with_retry(url, files_data, headers)
        
        result = response.json()
        
        self.logger.info("CSV files uploaded successfully", {
            "upload_id": result.get('uploadId'),
            "status": result.get('status'),
            "status_code": response.status_code
        })
        
        return result
    
    def _get_bearer_token(self) -> str:
        """
        Azure AD Bearer Tokenå–å¾—
        
        Returns:
            Bearer Token
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“å®Ÿè£…ã€æœ¬ç•ªã§ã¯ãƒ©ã‚¤ãƒ–ãƒ©ãƒªæ¨å¥¨ï¼‰
        if self._token_cache and self._is_token_valid():
            return self._token_cache['token']
        
        # Managed Identityã§ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚³ãƒ¼ãƒ—: https://management.azure.com/.default
        # å®Ÿéš›ã®ã‚¹ã‚³ãƒ¼ãƒ—ã¯APIä»•æ§˜ã«å¾“ã†
        token = self.credential.get_token("https://management.azure.com/.default")
        
        self._token_cache = {
            'token': token.token,
            'expires_on': token.expires_on
        }
        
        return token.token
    
    def _is_token_valid(self) -> bool:
        """ãƒˆãƒ¼ã‚¯ãƒ³æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯"""
        if not self._token_cache:
            return False
        
        import time
        # 5åˆ†ã®ãƒãƒƒãƒ•ã‚¡
        return time.time() < (self._token_cache['expires_on'] - 300)
    
    def _post_with_retry(
        self,
        url: str,
        files: List,
        headers: Dict,
        max_retries: int = 3
    ) -> requests.Response:
        """
        ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ä»˜ãPOST
        
        Exponential Backoff:
        - 1å›ç›®: å³åº§
        - 2å›ç›®: 2ç§’å¾…æ©Ÿ
        - 3å›ç›®: 4ç§’å¾…æ©Ÿ
        - 4å›ç›®: 8ç§’å¾…æ©Ÿ
        """
        for attempt in range(max_retries + 1):
            try:
                response = self.session.post(
                    url,
                    files=files,
                    headers=headers,
                    timeout=60  # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®ãŸã‚é•·ã‚ã«è¨­å®š
                )
                
                # æˆåŠŸ (2xx)
                if response.status_code < 300:
                    return response
                
                # 4xx ã‚¨ãƒ©ãƒ¼ï¼ˆãƒªãƒˆãƒ©ã‚¤ã—ãªã„ï¼‰
                if 400 <= response.status_code < 500:
                    self.logger.error("Client error, no retry", {
                        "status_code": response.status_code,
                        "response": response.text
                    })
                    response.raise_for_status()
                
                # 5xx ã‚¨ãƒ©ãƒ¼ï¼ˆãƒªãƒˆãƒ©ã‚¤ï¼‰
                if response.status_code >= 500 and attempt < max_retries:
                    wait_time = 2 ** attempt
                    self.logger.warning(f"Server error, retry in {wait_time}s", {
                        "attempt": attempt + 1,
                        "status_code": response.status_code
                    })
                    time.sleep(wait_time)
                    continue
                
                # æœ€çµ‚è©¦è¡Œã§å¤±æ•—
                response.raise_for_status()
                
            except requests.exceptions.RequestException as e:
                if attempt < max_retries:
                    wait_time = 2 ** attempt
                    self.logger.warning(f"Request failed, retry in {wait_time}s", {
                        "attempt": attempt + 1,
                        "error": str(e)
                    })
                    time.sleep(wait_time)
                    continue
                
                # æœ€çµ‚è©¦è¡Œã§å¤±æ•—
                self.logger.error("Request failed after all retries", {
                    "attempts": max_retries + 1,
                    "error": str(e)
                })
                raise
        
        raise Exception("Unexpected error in retry logic")
```

---

## ğŸ“Š JobMonitor Functionï¼ˆPythonç‰ˆï¼‰

### ã‚¯ãƒ©ã‚¹è¨­è¨ˆ

#### reporter.py
```python
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List
from shared.utils.azure_client import AzureTableClient, AzureBlobClient
from shared.utils.logger import StructuredLogger

class JobReporter:
    """ã‚¸ãƒ§ãƒ–ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    
    def __init__(self):
        self.table_client = AzureTableClient()
        self.blob_client = AzureBlobClient()
        self.logger = StructuredLogger("JobReporter")
    
    def generate_daily_report(self, target_date: datetime) -> Dict:
        """
        æ—¥æ¬¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        
        Args:
            target_date: å¯¾è±¡æ—¥ä»˜
            
        Returns:
            ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿
        """
        partition_key = target_date.strftime('%Y-%m')
        
        # å½“æ—¥ã®ã‚¸ãƒ§ãƒ–å–å¾—
        query = f"PartitionKey eq '{partition_key}'"
        entities = self.table_client.query_entities('JobHistory', query)
        
        jobs = [entity for entity in entities 
                if self._is_target_date(entity, target_date)]
        
        # çµ±è¨ˆè¨ˆç®—
        report = {
            'date': target_date.strftime('%Y-%m-%d'),
            'total_jobs': len(jobs),
            'completed': len([j for j in jobs if j['status'] == 'Completed']),
            'failed': len([j for j in jobs if j['status'] == 'Failed']),
            'processing': len([j for j in jobs if j['status'] == 'Processing']),
            'success_rate': 0.0,
            'avg_duration_minutes': 0.0,
            'total_records': 0,
            'python_jobs': 0,
            'javascript_jobs': 0
        }
        
        if report['total_jobs'] > 0:
            report['success_rate'] = (
                report['completed'] / report['total_jobs'] * 100
            )
        
        # å¹³å‡å‡¦ç†æ™‚é–“è¨ˆç®—
        completed_jobs = [j for j in jobs if j['status'] == 'Completed']
        if completed_jobs:
            durations = [
                (j['end_time'] - j['start_time']).total_seconds() / 60
                for j in completed_jobs
                if j.get('end_time')
            ]
            if durations:
                report['avg_duration_minutes'] = sum(durations) / len(durations)
        
        # ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°é›†è¨ˆ
        for job in jobs:
            if job.get('recordCounts'):
                import json
                counts = json.loads(job['recordCounts'])
                report['total_records'] += sum(counts.values())
            
            # è¨€èªåˆ¥é›†è¨ˆ
            if job.get('version') == 'python':
                report['python_jobs'] += 1
            elif job.get('version') == 'javascript':
                report['javascript_jobs'] += 1
        
        # CSVä¿å­˜
        self._save_report_csv(report, target_date)
        
        return report
    
    def _is_target_date(self, entity: Dict, target_date: datetime) -> bool:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒå¯¾è±¡æ—¥ä»˜ã‹åˆ¤å®š"""
        start_time = entity.get('start_time')
        if not start_time:
            return False
        
        if isinstance(start_time, str):
            start_time = datetime.fromisoformat(start_time.replace('Z', '+00:00'))
        
        return start_time.date() == target_date.date()
    
    def _save_report_csv(self, report: Dict, target_date: datetime):
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’CSVã¨ã—ã¦ä¿å­˜"""
        df = pd.DataFrame([report])
        csv_content = df.to_csv(index=False)
        
        blob_name = f"reports/daily/report-{target_date.strftime('%Y%m%d')}.csv"
        
        self.blob_client.upload_blob(
            container='oneroster-output',
            blob_name=blob_name,
            data=csv_content,
            content_type='text/csv'
        )
        
        self.logger.info("Daily report saved", {
            "date": target_date.strftime('%Y-%m-%d'),
            "blob_name": blob_name
        })
```

---

## ğŸ§© Shared Modulesï¼ˆå…±æœ‰ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼‰

### models/sds_models.py
```python
from dataclasses import dataclass
from typing import Optional

@dataclass
class SDSSchool:
    """SDSã‚¹ã‚¯ãƒ¼ãƒ«ãƒ¢ãƒ‡ãƒ«"""
    school_sis_id: str
    name: str
    school_number: Optional[str] = None
    school_nces_id: Optional[str] = None
    state_id: Optional[str] = None
    low_grade: Optional[str] = None
    high_grade: Optional[str] = None
    principal_sis_id: Optional[str] = None
    principal_name: Optional[str] = None
    principal_secondary_email: Optional[str] = None
    address: Optional[str] = None
    city: Optional[str] = None
    state: Optional[str] = None
    country: Optional[str] = None
    zip: Optional[str] = None
    phone: Optional[str] = None
    zone: Optional[str] = None

@dataclass
class SDSStudent:
    """SDSå­¦ç”Ÿãƒ¢ãƒ‡ãƒ«"""
    sis_id: str
    school_sis_id: str
    first_name: str
    last_name: str
    middle_name: Optional[str] = None
    grade: Optional[str] = None
    username: Optional[str] = None
    password: Optional[str] = None
    state_id: Optional[str] = None
    secondary_email: Optional[str] = None
    student_number: Optional[str] = None
    mailing_address: Optional[str] = None
    mailing_city: Optional[str] = None
    mailing_state: Optional[str] = None
    mailing_zip: Optional[str] = None
    mailing_country: Optional[str] = None
    residence_address: Optional[str] = None
    phone: Optional[str] = None
    mobile: Optional[str] = None
    graduation_year: Optional[str] = None
```

### utils/logger.py
```python
import logging
import json
from typing import Dict, Any
from datetime import datetime
from opencensus.ext.azure.log_exporter import AzureLogHandler

class StructuredLogger:
    """Application Insightsç”¨æ§‹é€ åŒ–ãƒ­ã‚°"""
    
    def __init__(self, component_name: str):
        self.component_name = component_name
        self.logger = logging.getLogger(component_name)
        
        # Application Insightsçµ±åˆ
        connection_string = os.getenv('APPLICATIONINSIGHTS_CONNECTION_STRING')
        if connection_string:
            handler = AzureLogHandler(connection_string=connection_string)
            self.logger.addHandler(handler)
        
        self.logger.setLevel(logging.INFO)
    
    def info(self, message: str, properties: Dict[str, Any] = None):
        """Infoãƒ¬ãƒ™ãƒ«ãƒ­ã‚°"""
        self._log(logging.INFO, message, properties)
    
    def warning(self, message: str, properties: Dict[str, Any] = None):
        """Warningãƒ¬ãƒ™ãƒ«ãƒ­ã‚°"""
        self._log(logging.WARNING, message, properties)
    
    def error(
        self,
        message: str,
        properties: Dict[str, Any] = None,
        exc_info: bool = False
    ):
        """Errorãƒ¬ãƒ™ãƒ«ãƒ­ã‚°"""
        self._log(logging.ERROR, message, properties, exc_info)
    
    def _log(
        self,
        level: int,
        message: str,
        properties: Dict[str, Any] = None,
        exc_info: bool = False
    ):
        """å†…éƒ¨ãƒ­ã‚°å‡¦ç†"""
        log_entry = {
            'timestamp': datetime.utcnow().isoformat() + 'Z',
            'component': self.component_name,
            'message': message,
            'properties': properties or {}
        }
        
        self.logger.log(
            level,
            json.dumps(log_entry),
            extra={'custom_dimensions': properties or {}},
            exc_info=exc_info
        )
```

---

## ğŸ“ æ¬¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [04_c4_component_diagram_javascript.md](./04_c4_component_diagram_javascript.md) - JavaScriptç‰ˆã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå›³

---

**æ–‡æ›¸ç®¡ç†è²¬ä»»è€…**: System Architect  
**æœ€çµ‚æ›´æ–°æ—¥**: 2025-10-27  
**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: Draft
